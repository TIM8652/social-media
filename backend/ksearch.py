import os
import json
import base64
import requests
from datetime import datetime
from apify_client import ApifyClient
from dotenv import load_dotenv
from translate import translate_post_by_id
from database import get_db_connection

load_dotenv()

# Apifyå®¢æˆ·ç«¯
from apiconfig import get_api_key

def get_apify_client():
    token = get_api_key("APIFY_API_TOKEN") or os.getenv("APIFY_API_TOKEN", "")
    return ApifyClient(token)

client = get_apify_client()

# DeepSeek APIé…ç½®
def get_deepseek_key():
    return get_api_key("get_deepseek_key()") or os.getenv("get_deepseek_key()", "")

DEEPSEEK_API_URL = "https://api.deepseek.com/v1/chat/completions"

def translate_keyword(keyword: str) -> str:
    """
    ä½¿ç”¨ DeepSeek API ç¿»è¯‘å…³é”®è¯åˆ°ä¸­æ–‡
    
    Args:
        keyword: è¦ç¿»è¯‘çš„å…³é”®è¯
    
    Returns:
        ç¿»è¯‘åçš„ä¸­æ–‡å…³é”®è¯
    """
    if not keyword or not keyword.strip():
        return ""
    
    if not get_deepseek_key():
        print("âš ï¸  get_deepseek_key() not configured, skipping translation")
        return ""
    
    try:
        print(f"  ğŸŒ ç¿»è¯‘å…³é”®è¯: {keyword}")
        
        headers = {
            "Authorization": f"Bearer {get_deepseek_key()}",
            "Content-Type": "application/json"
        }
        
        system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¿»è¯‘åŠ©æ‰‹ã€‚è¯·å°†ç”¨æˆ·æä¾›çš„æ ‡ç­¾/å…³é”®è¯ç¿»è¯‘æˆç®€ä½“ä¸­æ–‡ã€‚æ— è®ºæºè¯­è¨€æ˜¯è‹±è¯­ã€é˜¿æ‹‰ä¼¯è¯­è¿˜æ˜¯å…¶ä»–è¯­è¨€ï¼Œéƒ½è¯·ç¿»è¯‘æˆç®€ä½“ä¸­æ–‡ã€‚åªè¿”å›ç¿»è¯‘ç»“æœï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šã€å¼•å·æˆ–é¢å¤–å†…å®¹ã€‚ä¿æŒç®€æ´ã€‚"
        
        data = {
            "model": "deepseek-chat",
            "messages": [
                {
                    "role": "system",
                    "content": system_prompt
                },
                {
                    "role": "user",
                    "content": keyword
                }
            ],
            "temperature": 0.3,
            "max_tokens": 100
        }
        
        response = requests.post(DEEPSEEK_API_URL, headers=headers, json=data, timeout=30)
        
        if response.status_code == 200:
            result = response.json()
            translated = result['choices'][0]['message']['content'].strip()
            # ç§»é™¤å¯èƒ½çš„å¼•å·
            translated = translated.strip('"').strip("'")
            print(f"  âœ… ç¿»è¯‘å®Œæˆ: {keyword} -> {translated}")
            return translated
        else:
            print(f"  âš ï¸  ç¿»è¯‘å¤±è´¥: {response.status_code}")
            return ""
            
    except Exception as e:
        print(f"  âŒ ç¿»è¯‘é”™è¯¯: {e}")
        return ""

def check_search_exists(keyword):
    """æ£€æŸ¥æœç´¢æ ‡ç­¾æ˜¯å¦å·²å­˜åœ¨"""
    conn = get_db_connection()
    cursor = conn.cursor()
    cursor.execute('SELECT id FROM search WHERE keyword = %s', (keyword,))
    result = cursor.fetchone()
    cursor.close()
    conn.close()
    return result['id'] if result else None

def create_or_update_search(keyword):
    """åˆ›å»ºæˆ–æ›´æ–°æœç´¢æ ‡ç­¾"""
    conn = get_db_connection()
    cursor = conn.cursor()
    
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨
    cursor.execute('SELECT id, search_count, keyword_zh FROM search WHERE keyword = %s', (keyword,))
    result = cursor.fetchone()
    
    if result:
        # æ›´æ–°æœç´¢æ¬¡æ•°
        search_id = result['id']
        cursor.execute('''
            UPDATE search 
            SET search_count = search_count + 1,
                updated_at = CURRENT_TIMESTAMP
            WHERE id = %s
        ''', (search_id,))
        
        # å¦‚æœæ²¡æœ‰ä¸­æ–‡ç¿»è¯‘ï¼Œåˆ™ç¿»è¯‘
        if not result.get('keyword_zh'):
            keyword_zh = translate_keyword(keyword)
            if keyword_zh:
                cursor.execute('''
                    UPDATE search 
                    SET keyword_zh = %s
                    WHERE id = %s
                ''', (keyword_zh, search_id))
                print(f"  âœ… å·²æ›´æ–°å…³é”®è¯ç¿»è¯‘")
    else:
        # ç¿»è¯‘å…³é”®è¯
        keyword_zh = translate_keyword(keyword)
        
        # åˆ›å»ºæ–°è®°å½•
        cursor.execute('''
            INSERT INTO search (keyword, keyword_zh, search_count, total_posts)
            VALUES (%s, %s, 1, 0)
            RETURNING id
        ''', (keyword, keyword_zh))
        search_id = cursor.fetchone()['id']
    
    conn.commit()
    cursor.close()
    conn.close()
    
    return search_id

def download_image_to_base64(url):
    """ä¸‹è½½å›¾ç‰‡å¹¶è½¬æ¢ä¸ºBase64ï¼ˆå¤ç”¨é€»è¾‘ï¼‰"""
    try:
        response = requests.get(url, timeout=10)
        if response.status_code == 200:
            return base64.b64encode(response.content).decode('utf-8')
    except Exception as e:
        print(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥: {url}, é”™è¯¯: {e}")
    return None

def download_video_to_base64(url):
    """ä¸‹è½½è§†é¢‘å¹¶è½¬æ¢ä¸ºBase64ï¼ˆå¸¦è¶…æ—¶å’Œå¤§å°é™åˆ¶ï¼‰"""
    try:
        print(f"  æ­£åœ¨ä¸‹è½½è§†é¢‘: {url[:80]}...")
        # è®¾ç½®è¾ƒé•¿çš„è¶…æ—¶æ—¶é—´ï¼ˆè§†é¢‘æ–‡ä»¶è¾ƒå¤§ï¼‰
        response = requests.get(url, timeout=120, stream=True)
        if response.status_code == 200:
            # é™åˆ¶è§†é¢‘å¤§å°ï¼ˆä¾‹å¦‚æœ€å¤§100MBï¼‰
            max_size = 100 * 1024 * 1024  # 100MB
            video_data = b""
            downloaded_size = 0
            
            for chunk in response.iter_content(chunk_size=8192):
                video_data += chunk
                downloaded_size += len(chunk)
                
                # æ˜¾ç¤ºä¸‹è½½è¿›åº¦
                if downloaded_size % (1024 * 1024) == 0:  # æ¯1MBæ˜¾ç¤ºä¸€æ¬¡
                    print(f"    å·²ä¸‹è½½: {downloaded_size / 1024 / 1024:.1f}MB")
                
                if downloaded_size > max_size:
                    print(f"    âš ï¸ è§†é¢‘è¿‡å¤§(>{max_size/1024/1024}MB)ï¼Œè·³è¿‡: {url[:80]}")
                    return None
            
            print(f"    âœ… è§†é¢‘ä¸‹è½½å®Œæˆ: {downloaded_size / 1024 / 1024:.2f}MB")
            return base64.b64encode(video_data).decode('utf-8')
    except Exception as e:
        print(f"    âŒ ä¸‹è½½è§†é¢‘å¤±è´¥: {url[:80]}, é”™è¯¯: {e}")
    return None

def get_posts_urls_by_hashtag(keyword, limit=10, results_type="posts"):
    """
    ç¬¬ä¸€æ­¥ï¼šé€šè¿‡æ ‡ç­¾æœç´¢è·å–å¸–å­URLåˆ—è¡¨
    
    Args:
        keyword: æœç´¢å…³é”®è¯/æ ‡ç­¾
        limit: è¦è·å–çš„å¸–å­æ•°é‡
        results_type: "posts" æˆ– "stories"
    
    Returns:
        list: å¸–å­URLåˆ—è¡¨
    """
    print(f"\n{'='*60}")
    print(f"ç¬¬ä¸€æ­¥ï¼šæœç´¢æ ‡ç­¾ #{keyword}ï¼Œè·å– {limit} æ¡ {results_type} URL...")
    print(f"{'='*60}")
    
    run_input = {
        "hashtags": [keyword],
        "resultsType": results_type,
        "resultsLimit": limit,
        "keywordSearch": False,
    }
    
    try:
        # è°ƒç”¨ Instagram Hashtag Scraper
        run = client.actor("reGe1ST3OBgYZSsZJ").call(
            run_input=run_input,
            timeout_secs=180
        )
        
        # è·å–ç»“æœ
        urls = []
        for item in client.dataset(run["defaultDatasetId"]).iterate_items():
            if item.get('url'):
                urls.append(item['url'])
                print(f"  âœ… æ‰¾åˆ°: {item['url']}")
        
        print(f"\nğŸ“Š å…±æ‰¾åˆ° {len(urls)} æ¡URL")
        return urls
        
    except Exception as e:
        print(f"  âŒ APIè°ƒç”¨å¤±è´¥: {e}")
        return []

def get_post_details(urls):
    """
    ç¬¬äºŒæ­¥ï¼šæ ¹æ®URLåˆ—è¡¨è·å–å®Œæ•´çš„å¸–å­è¯¦æƒ…
    
    Args:
        urls: å¸–å­URLåˆ—è¡¨
    
    Returns:
        list: å®Œæ•´çš„å¸–å­æ•°æ®åˆ—è¡¨
    """
    if not urls:
        return []
    
    print(f"\n{'='*60}")
    print(f"ç¬¬äºŒæ­¥ï¼šè·å– {len(urls)} æ¡å¸–å­çš„å®Œæ•´è¯¦æƒ…...")
    print(f"{'='*60}")
    
    run_input = {
        "directUrls": urls,
        "resultsType": "posts",
        "resultsLimit": len(urls),
        "searchType": "hashtag",
        "searchLimit": 1,
        "addParentData": False,
    }
    
    try:
        # è°ƒç”¨ Instagram Scraper
        run = client.actor("shu8hvrXbJbY3Eb9W").call(
            run_input=run_input,
            timeout_secs=180
        )
        
        # è·å–å®Œæ•´æ•°æ®
        posts_data = []
        for item in client.dataset(run["defaultDatasetId"]).iterate_items():
            posts_data.append(item)
            post_type = item.get('type', 'Unknown')
            caption = item.get('caption', '')[:50] + '...' if item.get('caption') else 'No caption'
            print(f"  âœ… ç±»å‹: {post_type:10s} | {caption}")
        
        print(f"\nğŸ“Š å…±è·å– {len(posts_data)} æ¡å®Œæ•´å¸–å­æ•°æ®")
        return posts_data
        
    except Exception as e:
        print(f"  âŒ APIè°ƒç”¨å¤±è´¥: {e}")
        return []

def save_posts_to_db(posts, search_id):
    """
    ä¿å­˜å¸–å­åˆ°æ•°æ®åº“ï¼ˆä¸ cpostscrape.py ä¿æŒé«˜åº¦ä¸€è‡´ï¼‰
    
    Args:
        posts: å¸–å­æ•°æ®åˆ—è¡¨
        search_id: æœç´¢æ ‡ç­¾ID
    
    Returns:
        int: æˆåŠŸä¿å­˜çš„å¸–å­æ•°é‡
    """
    if not posts:
        return 0
    
    print(f"\n{'='*60}")
    print(f"ä¿å­˜ {len(posts)} æ¡å¸–å­åˆ°æ•°æ®åº“...")
    print(f"{'='*60}")
    
    conn = get_db_connection()
    cursor = conn.cursor()
    
    saved_count = 0
    
    for post in posts:
        try:
            post_id = post.get('id')
            post_type = post.get('type')  # åŸå§‹ç±»å‹ï¼šImage, Video, Sidecar
            
            # åˆå§‹åŒ–æ‰€æœ‰åª’ä½“ç›¸å…³å˜é‡
            display_url_base64 = None
            images_base64 = []
            video_url_base64 = None
            videos = []
            videos_base64 = []
            child_posts_order = []
            video_view_count = 0
            video_play_count = 0
            
            # ä¸‹è½½å°é¢å›¾ï¼ˆdisplayUrlï¼‰
            if post.get('displayUrl'):
                print(f"  ğŸ“¥ ä¸‹è½½å°é¢å›¾: {post['displayUrl'][:50]}...")
                display_url_base64 = download_image_to_base64(post['displayUrl'])
            
            # å¤„ç† Sidecar ç±»å‹ï¼ˆå¤šå›¾/æ··åˆç±»å‹ï¼‰
            if post_type == "Sidecar":
                child_posts = post.get('childPosts', [])
                has_video = False
                
                print(f"  å¤„ç† Sidecar å¸–å­ï¼ŒåŒ…å« {len(child_posts)} ä¸ªå­å¸–å­")
                
                for idx, child in enumerate(child_posts):
                    child_type = child.get('type')
                    print(f"    å­å¸–å­ {idx + 1}/{len(child_posts)}: ç±»å‹ = {child_type}")
                    
                    if child_type == "Video":
                        has_video = True
                        video_url = child.get('videoUrl')
                        if video_url:
                            videos.append(video_url)
                            # ä¸‹è½½è§†é¢‘è½¬Base64
                            video_base64 = download_video_to_base64(video_url)
                            if video_base64:
                                videos_base64.append(video_base64)
                            else:
                                videos_base64.append(None)  # ä¸‹è½½å¤±è´¥ï¼Œå ä½
                            
                            # è®°å½•é¡ºåº
                            child_posts_order.append({
                                "index": idx,
                                "type": "Video",
                                "ref": len(videos) - 1,
                                "short_code": child.get('shortCode'),
                                "video_view_count": child.get('videoViewCount'),
                                "video_duration": child.get('videoDuration')
                            })
                    
                    elif child_type == "Image":
                        img_url = child.get('displayUrl')
                        if img_url:
                            img_base64 = download_image_to_base64(img_url)
                            if img_base64:
                                images_base64.append(img_base64)
                            else:
                                images_base64.append(None)  # ä¸‹è½½å¤±è´¥ï¼Œå ä½
                            
                            # è®°å½•é¡ºåº
                            child_posts_order.append({
                                "index": idx,
                                "type": "Image",
                                "ref": len(images_base64) - 1,
                                "short_code": child.get('shortCode')
                            })
                
                # å¦‚æœåŒ…å«è§†é¢‘ï¼Œä¿®æ”¹post_typeä¸º Sidecar_video
                if has_video:
                    post_type = "Sidecar_video"
                    print(f"  âœ… æ£€æµ‹åˆ°æ··åˆç±»å‹ï¼Œpost_type æ›´æ”¹ä¸º: Sidecar_video")
            
            # å¤„ç†çº¯ Video ç±»å‹ï¼ˆå•è§†é¢‘ï¼‰
            elif post_type == "Video":
                video_url = post.get('videoUrl')
                if video_url:
                    print(f"  ğŸ“¥ ä¸‹è½½è§†é¢‘: {video_url[:50]}...")
                    video_url_base64 = download_video_to_base64(video_url)
                # è·å–è§†é¢‘è§‚çœ‹æ•°å’Œæ’­æ”¾æ•°
                video_view_count = post.get('videoViewCount', 0)
                video_play_count = post.get('videoPlayCount', 0)
            
            # å¤„ç†çº¯ Image ç±»å‹ï¼ˆä¸éœ€è¦é¢å¤–å¤„ç†ï¼Œåªæœ‰å°é¢å›¾ï¼‰
            # displayUrl å·²ç»åœ¨ä¸Šé¢å¤„ç†äº†
            
            # å¤„ç†è¯„è®ºæ•°æ®
            latest_comments = post.get('latestComments', [])
            # ä¼˜å…ˆä½¿ç”¨æŠ“å–çš„ firstComment å­—æ®µï¼Œå…¶æ¬¡å›é€€åˆ° latestComments çš„ç¬¬ä¸€æ¡
            first_comment_text = post.get('firstComment') or None
            try:
                if not first_comment_text and isinstance(latest_comments, list) and latest_comments:
                    first = latest_comments[0]
                    if isinstance(first, dict):
                        first_comment_text = first.get('text')
                    elif isinstance(first, str):
                        first_comment_text = first
            except Exception:
                first_comment_text = None
            
            # æ’å…¥æˆ–æ›´æ–°æ•°æ®ï¼ˆä½¿ç”¨ ON CONFLICT è¦†ç›–ï¼‰
            cursor.execute('''
                INSERT INTO post_data (
                    post_id, post_type, short_code, url, input_url,
                    caption, alt,
                    hashtags, mentions,
                    comments_count, likes_count, is_comments_disabled,
                    latest_comments, first_comment,
                    dimensions_height, dimensions_width,
                    display_url, display_url_base64,
                    video_url, video_url_base64, video_duration,
                    video_view_count, video_play_count,
                    images, images_base64, child_posts,
                    videos, videos_base64, child_posts_order,
                    owner_id, owner_username, owner_full_name,
                    timestamp, is_pinned, is_sponsored, product_type,
                    search_id, competitor_id
                ) VALUES (
                    %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,
                    %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,
                    %s, %s, %s, %s, %s, %s, %s, %s, NULL
                )
                ON CONFLICT (post_id) DO UPDATE SET
                    post_type = EXCLUDED.post_type,
                    short_code = EXCLUDED.short_code,
                    url = EXCLUDED.url,
                    input_url = EXCLUDED.input_url,
                    caption = EXCLUDED.caption,
                    alt = EXCLUDED.alt,
                    hashtags = EXCLUDED.hashtags,
                    mentions = EXCLUDED.mentions,
                    comments_count = EXCLUDED.comments_count,
                    likes_count = EXCLUDED.likes_count,
                    is_comments_disabled = EXCLUDED.is_comments_disabled,
                    latest_comments = EXCLUDED.latest_comments,
                    first_comment = EXCLUDED.first_comment,
                    dimensions_height = EXCLUDED.dimensions_height,
                    dimensions_width = EXCLUDED.dimensions_width,
                    display_url = EXCLUDED.display_url,
                    display_url_base64 = EXCLUDED.display_url_base64,
                    video_url = EXCLUDED.video_url,
                    video_url_base64 = EXCLUDED.video_url_base64,
                    video_duration = EXCLUDED.video_duration,
                    video_view_count = EXCLUDED.video_view_count,
                    video_play_count = EXCLUDED.video_play_count,
                    images = EXCLUDED.images,
                    images_base64 = EXCLUDED.images_base64,
                    child_posts = EXCLUDED.child_posts,
                    videos = EXCLUDED.videos,
                    videos_base64 = EXCLUDED.videos_base64,
                    child_posts_order = EXCLUDED.child_posts_order,
                    owner_id = EXCLUDED.owner_id,
                    owner_username = EXCLUDED.owner_username,
                    owner_full_name = EXCLUDED.owner_full_name,
                    timestamp = EXCLUDED.timestamp,
                    is_pinned = EXCLUDED.is_pinned,
                    is_sponsored = EXCLUDED.is_sponsored,
                    product_type = EXCLUDED.product_type,
                    search_id = EXCLUDED.search_id,
                    updated_at = NOW()
                RETURNING id
            ''', (
                post_id,
                post_type,  # å¯èƒ½æ˜¯ "Sidecar_video"
                post.get('shortCode'),
                post.get('url'),
                post.get('inputUrl'),
                post.get('caption'),
                post.get('alt'),
                json.dumps(post.get('hashtags', [])),
                json.dumps(post.get('mentions', [])),
                post.get('commentsCount', 0),
                post.get('likesCount', 0),
                post.get('isCommentsDisabled', False),
                json.dumps(latest_comments),
                first_comment_text,
                post.get('dimensionsHeight'),
                post.get('dimensionsWidth'),
                post.get('displayUrl'),
                display_url_base64,
                post.get('videoUrl'),
                video_url_base64,
                post.get('videoDuration'),
                video_view_count,
                video_play_count,
                json.dumps(post.get('images', [])),
                json.dumps(images_base64) if images_base64 else None,
                json.dumps(post.get('childPosts', [])),
                json.dumps(videos) if videos else None,
                json.dumps(videos_base64) if videos_base64 else None,
                json.dumps(child_posts_order) if child_posts_order else None,
                post.get('ownerId'),
                post.get('ownerUsername'),
                post.get('ownerFullName'),
                post.get('timestamp'),
                post.get('isPinned', False),
                post.get('isSponsored', False),
                post.get('productType'),
                search_id
            ))
            
            # è·å–æ’å…¥/æ›´æ–°åçš„æ•°æ®åº“ID
            db_id = cursor.fetchone()['id']
            
            conn.commit()
            saved_count += 1
            
            print(f"  âœ… ä¿å­˜æˆåŠŸ: {post_id} (DB ID: {db_id})")
            
            # è§¦å‘ç¿»è¯‘ï¼ˆä½¿ç”¨æ•°æ®åº“IDï¼‰
            print(f"  ğŸŒ è§¦å‘ç¿»è¯‘: DB ID {db_id}")
            translate_post_by_id(db_id)
            
        except Exception as e:
            print(f"  âŒ ä¿å­˜å¤±è´¥: {post.get('id', 'Unknown')}, é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            conn.rollback()
            continue
    
    # æ›´æ–° search è¡¨çš„ total_posts è®¡æ•°
    cursor.execute('''
        UPDATE search 
        SET total_posts = (
            SELECT COUNT(*) FROM post_data WHERE search_id = %s
        )
        WHERE id = %s
    ''', (search_id, search_id))
    conn.commit()
    
    cursor.close()
    conn.close()
    
    print(f"\nâœ… æˆåŠŸä¿å­˜ {saved_count} æ¡å¸–å­")
    return saved_count

def scrape_by_keyword(keyword, post_count, scrape_type="posts"):
    """
    ä¸»å‡½æ•°ï¼šæ ¹æ®å…³é”®è¯æŠ“å–æ•°æ®
    
    Args:
        keyword: æœç´¢å…³é”®è¯/æ ‡ç­¾
        post_count: è¦æŠ“å–çš„æ•°é‡
        scrape_type: "posts" / "stories" / "both"
    
    Returns:
        dict: æŠ“å–ç»“æœ
    """
    print(f"\n{'='*60}")
    print(f"å¼€å§‹æœç´¢æŠ“å–")
    print(f"å…³é”®è¯: {keyword}")
    print(f"æ•°é‡: {post_count}")
    print(f"ç±»å‹: {scrape_type}")
    print(f"{'='*60}\n")
    
    try:
        # åˆ›å»ºæˆ–æ›´æ–°æœç´¢è®°å½•
        search_id = create_or_update_search(keyword)
        print(f"âœ… æœç´¢æ ‡ç­¾ID: {search_id}")
        
        all_posts = []
        
        # æ ¹æ®ç±»å‹æŠ“å–
        if scrape_type in ["posts", "both"]:
            print(f"\nğŸ“ æŠ“å– posts ç±»å‹...")
            posts_urls = get_posts_urls_by_hashtag(keyword, post_count, "posts")
            if posts_urls:
                posts_data = get_post_details(posts_urls)
                all_posts.extend(posts_data)
        
        if scrape_type in ["stories", "both"]:
            print(f"\nğŸ¥ æŠ“å– stories ç±»å‹...")
            stories_urls = get_posts_urls_by_hashtag(keyword, post_count, "stories")
            if stories_urls:
                stories_data = get_post_details(stories_urls)
                all_posts.extend(stories_data)
        
        if not all_posts:
            return {
                "success": False,
                "message": "æœªæŠ“å–åˆ°ä»»ä½•æ•°æ®"
            }
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        saved_count = save_posts_to_db(all_posts, search_id)
        
        return {
            "success": True,
            "message": f"æˆåŠŸæŠ“å–å¹¶ä¿å­˜ {saved_count} æ¡å¸–å­",
            "post_count": saved_count,
            "search_id": search_id
        }
        
    except Exception as e:
        print(f"\nâŒ æŠ“å–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return {
            "success": False,
            "message": f"æŠ“å–å¤±è´¥: {str(e)}"
        }

if __name__ == "__main__":
    # æµ‹è¯•
    result = scrape_by_keyword("ØªØ¹Ù„Ù…_Ø§Ù„Ø§Ù†Ø¬Ù„ÙŠØ²ÙŠØ©", 2, "posts")
    print(f"\næœ€ç»ˆç»“æœ: {result}")

